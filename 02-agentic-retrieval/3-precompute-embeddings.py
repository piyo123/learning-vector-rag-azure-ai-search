# Generated by GitHub Copilot Feb. 2, 2026

import os
import numpy as np
import json
import requests
import io
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import SearchField, SearchIndex
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient
from datetime import datetime
from pypdf import PdfReader

# Load environment variables
load_dotenv()

SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AOAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AOAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AOAI_EMBEDDING_MODEL = "text-embedding-3-small"
AOAI_EMBEDDING_DEPLOYMENT = "text-embedding-3-small"

BLOB_CONNECTION_STRING = os.getenv("BLOB_CONNECTION_STRING")
BLOB_CONTAINER_NAME = os.getenv("BLOB_CONTAINER_NAME")
BLOB_FOLDER_NAME = os.getenv("BLOB_FOLDER_NAME", "")

INDEX_NAME = "stories-index"
CHUNK_SIZE = 1000  # Maximum characters per chunk

# Initialize clients
index_client = SearchIndexClient(endpoint=SEARCH_ENDPOINT, credential=AzureKeyCredential(SEARCH_API_KEY))
search_client = SearchClient(endpoint=SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=AzureKeyCredential(SEARCH_API_KEY))
blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)

def extract_text_from_pdf(pdf_bytes: bytes) -> list[str]:
    """
    Extract text from PDF bytes, returning one string per page.
    """
    pdf_file = io.BytesIO(pdf_bytes)
    reader = PdfReader(pdf_file)
    pages = []
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text.strip():  # Only add non-empty pages
            pages.append(page_text)
    return pages


def chunk_text(text: str, chunk_size: int = CHUNK_SIZE) -> list[str]:
    """
    Split text into chunks of approximately chunk_size characters.
    """
    chunks = []
    current_chunk = ""
    
    # Split by paragraphs (double newline)
    paragraphs = text.split("\n\n")
    
    for paragraph in paragraphs:
        # If adding this paragraph exceeds chunk size, save current chunk
        if len(current_chunk) + len(paragraph) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    # Add the last chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def extract_title_from_text(text: str) -> str:
    """
    Extract title from text (first non-empty line).
    """
    lines = text.strip().split("\n")
    for line in lines:
        line = line.strip()
        if line:
            return line[:100]  # Limit to 100 characters
    return "Untitled"


def get_embeddings(text: str) -> list:
    """
    Get embeddings from Azure OpenAI REST API and convert to float32 (Single type).
    This avoids JSON Double serialization issues.
    """
    url = f"{AOAI_ENDPOINT}/openai/deployments/{AOAI_EMBEDDING_DEPLOYMENT}/embeddings?api-version=2024-08-01-preview"
    
    headers = {
        "Content-Type": "application/json",
        "api-key": AOAI_API_KEY
    }
    
    payload = {
        "input": text
    }
    
    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    
    data = response.json()
    embedding_vector = data['data'][0]['embedding']
    
    # Convert to float32 (Edm.Single) array
    embeddings = np.array(embedding_vector, dtype=np.float32)
    # Convert back to list of native Python floats (float32)
    return embeddings.tolist()


def upload_documents_with_precomputed_embeddings():
    """
    Read documents from Blob Storage, compute embeddings with proper typing, and upload to index.
    """
    
    container_client = blob_service_client.get_container_client(BLOB_CONTAINER_NAME)
    
    # List all PDF blobs in the specified folder
    prefix = BLOB_FOLDER_NAME + "/" if BLOB_FOLDER_NAME else ""
    blobs = container_client.list_blobs(name_starts_with=prefix)
    
    all_documents = []
    doc_counter = 0
    
    for blob in blobs:
        # Only process PDF files
        if not blob.name.lower().endswith('.pdf'):
            continue
        
        print(f"\nProcessing: {blob.name}")
        
        # Download blob
        blob_client = container_client.get_blob_client(blob.name)
        pdf_bytes = blob_client.download_blob().readall()
        
        # Extract text (one string per page)
        pages = extract_text_from_pdf(pdf_bytes)
        if not pages:
            print(f"  ‚ö†Ô∏è No text found in {blob.name}")
            continue
        
        # Extract title from first page
        story_title = extract_title_from_text(pages[0])
        
        print(f"  üìÑ Extracted {len(pages)} pages")
        
        # Create documents for each page (1 page = 1 chunk)
        for page_num, page_text in enumerate(pages, start=1):
            if not page_text.strip():
                continue
            
            doc_counter += 1
            doc_id = f"doc{doc_counter}"
            
            print(f"  üî¢ Computing embeddings for page {page_num}/{len(pages)}...")
            embeddings = get_embeddings(page_text)
            
            document = {
                "id": doc_id,
                "page_chunk": page_text,
                "page_embeddings": embeddings,
                "story_title": story_title,
                "source_file": blob.name.split('/')[-1],
                "source_path": blob.name,
                "parent_id": "",
                "last_modified": blob.last_modified.isoformat() if blob.last_modified else datetime.now(datetime.timezone.utc).isoformat() + "Z"
            }
            
            all_documents.append(document)
    
    if not all_documents:
        print("\n‚ö†Ô∏è No documents to upload")
        return
    
    # Upload documents in batches (Azure Search recommends batches of 1000)
    batch_size = 1000
    print(f"\nüì§ Uploading {len(all_documents)} documents to index...")
    
    for i in range(0, len(all_documents), batch_size):
        batch = all_documents[i:i + batch_size]
        try:
            results = search_client.upload_documents(documents=batch)
            success_count = sum(1 for r in results if r.succeeded)
            print(f"  ‚úÖ Batch {i//batch_size + 1}: {success_count}/{len(batch)} documents uploaded")
        except Exception as e:
            print(f"  ‚ùå Error uploading batch {i//batch_size + 1}: {e}")
            raise
    
    print(f"\n‚úÖ Total uploaded: {len(all_documents)} documents")

def verify_embeddings():
    """
    Retrieve a document and verify embeddings are present and correct type.
    """
    print("\nVerifying embeddings...")
    try:
        results = list(search_client.search('*', top=1))
        if results:
            doc = results[0]
            print(f"‚úÖ Retrieved document: {doc['id']}")
            if 'page_embeddings' in doc:
                embeddings = doc['page_embeddings']
                print(f"   - Embeddings present: {len(embeddings)} dimensions")
                print(f"   - First 5 values: {embeddings[:5]}")
                print(f"   - Type: {type(embeddings[0]) if embeddings else 'N/A'}")
            else:
                print(f"   - ‚ùå No embeddings found, ‚úÖbut this is expected behavior. No Problem.")
        else:
            print("No documents found")
    except Exception as e:
        print(f"‚ùå Error verifying: {e}")

if __name__ == "__main__":
    print("=== Precomputed Embeddings with Blob Storage Integration ===\n")
    print("This method:")
    print("1. Reads PDF files from Azure Blob Storage")
    print("2. Extracts text and chunks into manageable sizes")
    print("3. Computes embeddings with proper float32 (Single) typing")
    print("4. Uploads documents directly to Azure Search index")
    print("5. Avoids the Double‚ÜíSingle conversion problem\n")
    
    print(f"Container: {BLOB_CONTAINER_NAME}")
    print(f"Folder: {BLOB_FOLDER_NAME or '(root)'}")
    print(f"Index: {INDEX_NAME}\n")
    
    upload_documents_with_precomputed_embeddings()
    verify_embeddings()
    
    print("\n‚úÖ Precomputed embeddings approach complete!")
